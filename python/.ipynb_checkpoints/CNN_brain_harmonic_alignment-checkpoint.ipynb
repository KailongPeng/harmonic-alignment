{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc085b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy\n",
    "# !pip3 install tasklogger\n",
    "# !pip3 install joblib\n",
    "# !pip3 install graphtools\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b725bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib,os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def shuffleRows(arr1):\n",
    "    arr1=list(arr1)\n",
    "    random.shuffle(arr1)\n",
    "    arr1=np.asarray(arr1)\n",
    "    return arr1\n",
    "\n",
    "class Scaling:\n",
    "    def fit(self, train=0, test=0):\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler()  # z = (x - u) / s\n",
    "        self.scaler.fit(train)\n",
    "        self.pcs_train = self.scaler.transform(train) # equivelent with stats.zscore(b, axis=0)\n",
    "        if type(test)!=int:\n",
    "            self.pcs_test = self.scaler.transform(test)\n",
    "\n",
    "class activation_PCA:\n",
    "    def fit(self, X_train=0, X_test=0, num_components=1000):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.num_components = min(n_samples, n_features) if num_components > min(n_samples, n_features) else num_components\n",
    "        self.model = PCA(self.num_components)\n",
    "        self.model.fit(X_train)\n",
    "        # print('explained_variance_ratio=',self.model.explained_variance_ratio_)\n",
    "        # principle components are the coefficients that transform original data into principal vectors space.\n",
    "        self.pcs_train = self.model.transform(X_train)\n",
    "        if type(X_test)!=int:\n",
    "            self.pcs_test = self.model.transform(X_test)\n",
    "\n",
    "def zscoreForEachRow(data):\n",
    "    data=data.T\n",
    "    return ((data-np.mean(data,0))/np.std(data,0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01cd0da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data.shape=(3107, 25088)\n",
      "brain_data.shape=(3107, 20251)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "subject = 'CSI2'\n",
    "model_dir='/gpfs/milgram/project/turk-browne/projects/rtSynth/features/shareWithSmita/model/'\n",
    "brain_dir='/gpfs/milgram/project/turk-browne/projects/rtSynth/features/shareWithSmita/brain/'\n",
    "model_data = np.load(model_dir+'VGG_b5p_3107.npy')\n",
    "brain_data = np.load(f'{brain_dir}OT_3107_{subject}_stdin.npy')\n",
    "brain_data = np.transpose(brain_data)\n",
    "print(f\"model_data.shape={model_data.shape}\")\n",
    "print(f\"brain_data.shape={brain_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42bfc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_PCA:\n",
    "    def fit(self, X_train=0, X_test=0, num_components=1000):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.num_components = min(n_samples, n_features) if num_components > min(n_samples, n_features) else num_components\n",
    "        self.model = PCA(self.num_components)\n",
    "        self.model.fit(X_train)\n",
    "        # print('explained_variance_ratio=',self.model.explained_variance_ratio_)\n",
    "        # principle components are the coefficients that transform original data into principal vectors space.\n",
    "        self.pcs_train = self.model.transform(X_train)\n",
    "        if type(X_test)!=int:\n",
    "            self.pcs_test = self.model.transform(X_test)\n",
    "            \n",
    "PCA1=activation_PCA()\n",
    "PCA1.fit(X_train=model_data)\n",
    "PCA2=activation_PCA()\n",
    "PCA2.fit(X_train=brain_data)\n",
    "\n",
    "model_data = PCA1.pcs_train\n",
    "brain_data = PCA2.pcs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b47ff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data.shape=(3107, 1000)\n",
      "brain_data.shape=(3107, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_data.shape={model_data.shape}\")\n",
    "print(f\"brain_data.shape={brain_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe46806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5254,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 图片分属的session id\n",
    "_sessionRun=np.load(f'/gpfs/milgram/project/turk-browne/projects/rtSynth/sessionRun/subject_{subject}.npy')\n",
    "_sessionRun.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1065f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/milgram/project/turk-browne/kp578/conda_envs/harmonic/lib/python3.9/site-packages/graphtools/graphs.py:287: RuntimeWarning: Detected zero distance between 109 pairs of samples. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  warnings.warn(\n",
      "2021-07-26 03:36:17,591:[WARNING](pygsp.graphs.graph.compute_fourier_basis): Computing the full eigendecomposition of a large matrix (3107 x 3107) may take some time.\n",
      "2021-07-26 03:36:28,485:[WARNING](pygsp.graphs.graph.compute_fourier_basis): Computing the full eigendecomposition of a large matrix (3107 x 3107) may take some time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_filters = 8\n",
    "align_op = harmonicalignment.HarmonicAlignment(\n",
    "            int(n_filters),\n",
    "            t=1, # 1\n",
    "            overlap=4,\n",
    "            verbose=0,\n",
    "            knn_X=20, # 20\n",
    "            knn_Y=20, # 20\n",
    "            knn_XY=10, # 10\n",
    "            decay_X=20, # 20\n",
    "            decay_Y=20, # 20\n",
    "            decay_XY=10, # 10\n",
    "            n_pca_X=100, # 100\n",
    "            n_pca_Y=100, # 100\n",
    "            n_pca_XY=None, # None\n",
    "        )\n",
    "align_op.align(model_data, brain_data)\n",
    "XY_aligned = align_op.diffusion_map()\n",
    "\n",
    "model_data_aligned=XY_aligned[:model_data.shape[0]]\n",
    "brain_data_aligned=XY_aligned[model_data.shape[0]:]\n",
    "\n",
    "_overlapRatio = overlapRatio(xb1_aligned,xb2_aligned,title=f'n_filters={n_filters}')\n",
    "print(f\"n_filters={n_filters}, overlapRatio={_overlapRatio}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别作图\n",
    "plt.figure()\n",
    "plt.scatter(model_data_aligned[:,0],model_data_aligned[:,1])\n",
    "plt.title(f'model_data_aligned ; n_filters={n_filters}')\n",
    "plt.figure()\n",
    "plt.scatter(brain_data_aligned[:,0],brain_data_aligned[:,1])\n",
    "plt.title(f'brain_data_aligned ; n_filters={n_filters}')\n",
    "plt.figure()\n",
    "plt.scatter(XY_aligned[:,0],XY_aligned[:,1])\n",
    "plt.title(f'x1x2 aligned ; n_filters={n_filters}')\n",
    "# 发现效果非常差，因为原始数据是瑞士卷的数据的原始版x1和放大版x2\n",
    "# 但是我得到的xb1_aligned 和 xb2_aligned 看到的效果是完全不重合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a88a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
